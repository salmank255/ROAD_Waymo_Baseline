{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is for domain adaptation\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import argparse\n",
    "import numpy as np\n",
    "from modules import utils\n",
    "from train import train\n",
    "from data import VideoDataset\n",
    "from torchvision import transforms\n",
    "import data.transforms as vtf\n",
    "from models.retinanet import build_retinanet\n",
    "from gen_dets import gen_dets, eval_framewise_dets\n",
    "from tubes import build_eval_tubes\n",
    "from val import val\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "def str2bool(v):\n",
    "    return v.lower() in (\"yes\", \"true\", \"t\", \"1\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Training single stage FPN with OHEM, resnet as backbone')\n",
    "    parser.add_argument('DATA_ROOT', help='Location to root directory for dataset reading') # /mnt/mars-fast/datasets/\n",
    "    parser.add_argument('SAVE_ROOT', help='Location to root directory for saving checkpoint models') # /mnt/mars-alpha/\n",
    "    parser.add_argument('MODEL_PATH',help='Location to root directory where kinetics pretrained models are stored')\n",
    "    parser.add_argument('--ANNO_ROOT', default='', help='Location to directory where annotations are stored') # /mnt/mars-fast/datasets/\n",
    "    parser.add_argument('--MODE', default='train',\n",
    "                        help='MODE can be train, gen_dets, eval_frames, eval_tubes define SUBSETS accordingly, build tubes')\n",
    "    # Name of backbone network, e.g. resnet18, resnet34, resnet50, resnet101 resnet152 are supported\n",
    "    parser.add_argument('--ARCH', default='resnet50', \n",
    "                        type=str, help=' base arch')\n",
    "    parser.add_argument('--MODEL_TYPE', default='I3D',\n",
    "                        type=str, help=' base model')\n",
    "    parser.add_argument('--ANCHOR_TYPE', default='RETINA',\n",
    "                        type=str, help='type of anchors to be used in model')\n",
    "    \n",
    "    parser.add_argument('--SEQ_LEN', default=8,\n",
    "                        type=int, help='NUmber of input frames')\n",
    "    parser.add_argument('--TEST_SEQ_LEN', default=8,\n",
    "                        type=int, help='NUmber of input frames')\n",
    "    parser.add_argument('--MIN_SEQ_STEP', default=1,\n",
    "                        type=int, help='DIFFERENCE of gap between the frames of sequence')\n",
    "    parser.add_argument('--MAX_SEQ_STEP', default=1,\n",
    "                        type=int, help='DIFFERENCE of gap between the frames of sequence')\n",
    "    # if output heads are have shared features or not: 0 is no-shareing else sharining enabled\n",
    "    # parser.add_argument('--MULIT_SCALE', default=False, type=str2bool,help='perfrom multiscale training')\n",
    "    parser.add_argument('--HEAD_LAYERS', default=3, \n",
    "                        type=int,help='0 mean no shareding more than 0 means shareing')\n",
    "    parser.add_argument('--NUM_FEATURE_MAPS', default=5, \n",
    "                        type=int,help='0 mean no shareding more than 0 means shareing')\n",
    "    parser.add_argument('--CLS_HEAD_TIME_SIZE', default=3, \n",
    "                        type=int, help='Temporal kernel size of classification head')\n",
    "    parser.add_argument('--REG_HEAD_TIME_SIZE', default=3,\n",
    "                    type=int, help='Temporal kernel size of regression head')\n",
    "    \n",
    "    #  Name of the dataset only voc or coco are supported\n",
    "    parser.add_argument('--DATASET', default='road', \n",
    "                        type=str,help='dataset being used')\n",
    "    parser.add_argument('--TRAIN_SUBSETS', default='train,', \n",
    "                        type=str,help='Training SUBSETS seprated by ,')\n",
    "    parser.add_argument('--VAL_SUBSETS', default='val', \n",
    "                        type=str,help='Validation SUBSETS seprated by ,')\n",
    "    parser.add_argument('--TEST_SUBSETS', default='', \n",
    "                        type=str,help='Testing SUBSETS seprated by ,')\n",
    "    # Input size of image only 600 is supprted at the moment \n",
    "    parser.add_argument('--MIN_SIZE', default=960, \n",
    "                        type=int, help='Input Size for FPN')\n",
    "    \n",
    "    #  data loading argumnets\n",
    "    parser.add_argument('-b','--BATCH_SIZE', default=4, \n",
    "                        type=int, help='Batch size for training')\n",
    "    parser.add_argument('--TEST_BATCH_SIZE', default=1, \n",
    "                        type=int, help='Batch size for testing')\n",
    "    # Number of worker to load data in parllel\n",
    "    parser.add_argument('--NUM_WORKERS', '-j', default=8, \n",
    "                        type=int, help='Number of workers used in dataloading')\n",
    "    # optimiser hyperparameters\n",
    "    parser.add_argument('--OPTIM', default='SGD', \n",
    "                        type=str, help='Optimiser type')\n",
    "    parser.add_argument('--RESUME', default=0, \n",
    "                        type=int, help='Resume from given epoch')\n",
    "    parser.add_argument('--MAX_EPOCHS', default=30, \n",
    "                        type=int, help='Number of training epoc')\n",
    "    parser.add_argument('-l','--LR', '--learning-rate', \n",
    "                        default=0.004225, type=float, help='initial learning rate')\n",
    "    parser.add_argument('--MOMENTUM', default=0.9, \n",
    "                        type=float, help='momentum')\n",
    "    parser.add_argument('--MILESTONES', default='20,25', \n",
    "                        type=str, help='Chnage the lr @')\n",
    "    parser.add_argument('--GAMMA', default=0.1, \n",
    "                        type=float, help='Gamma update for SGD')\n",
    "    parser.add_argument('--WEIGHT_DECAY', default=1e-4, \n",
    "                        type=float, help='Weight decay for SGD')\n",
    "    \n",
    "    # Freeze layers or not \n",
    "    parser.add_argument('--FBN','--FREEZE_BN', default=True, \n",
    "                        type=str2bool, help='freeze bn layers if true or else keep updating bn layers')\n",
    "    parser.add_argument('--FREEZE_UPTO', default=1, \n",
    "                        type=int, help='layer group number in ResNet up to which needs to be frozen')\n",
    "    \n",
    "    # Loss function matching threshold\n",
    "    parser.add_argument('--POSTIVE_THRESHOLD', default=0.5, \n",
    "                        type=float, help='Min threshold for Jaccard index for matching')\n",
    "    parser.add_argument('--NEGTIVE_THRESHOLD', default=0.4,\n",
    "                        type=float, help='Max threshold Jaccard index for matching')\n",
    "    # Evaluation hyperparameters\n",
    "    parser.add_argument('--EVAL_EPOCHS', default='30', \n",
    "                        type=str, help='eval epochs to test network on these epoch checkpoints usually the last epoch is used')\n",
    "    parser.add_argument('--VAL_STEP', default=1, \n",
    "                        type=int, help='Number of training epoch before evaluation')\n",
    "    parser.add_argument('--IOU_THRESH', default=0.5, \n",
    "                        type=float, help='Evaluation threshold for validation and for frame-wise mAP')\n",
    "    parser.add_argument('--CONF_THRESH', default=0.025, \n",
    "                        type=float, help='Confidence threshold for to remove detection below given number')\n",
    "    parser.add_argument('--NMS_THRESH', default=0.5, \n",
    "                        type=float, help='NMS threshold to apply nms at the time of validation')\n",
    "    parser.add_argument('--TOPK', default=10, \n",
    "                        type=int, help='topk detection to keep for evaluation')\n",
    "    parser.add_argument('--GEN_CONF_THRESH', default=0.025, \n",
    "                        type=float, help='Confidence threshold at the time of generation and dumping')\n",
    "    parser.add_argument('--GEN_TOPK', default=100, \n",
    "                        type=int, help='topk at the time of generation')\n",
    "    parser.add_argument('--GEN_NMS', default=0.5, \n",
    "                        type=float, help='NMS at the time of generation')\n",
    "    parser.add_argument('--CLASSWISE_NMS', default=False, \n",
    "                        type=str2bool, help='apply classwise NMS/no tested properly')\n",
    "    parser.add_argument('--JOINT_4M_MARGINALS', default=False, \n",
    "                        type=str2bool, help='generate score of joints i.e. duplexes or triplet by marginals like agents and actions scores')\n",
    "    \n",
    "    ## paths hyper parameters\n",
    "    parser.add_argument('--COMPUTE_PATHS', default=False, \n",
    "                        type=str2bool, help=' COMPUTE_PATHS if set true then it overwrite existing ones')\n",
    "    parser.add_argument('--PATHS_IOUTH', default=0.5,\n",
    "                        type=float, help='Iou threshold for building paths to limit neighborhood search')\n",
    "    parser.add_argument('--PATHS_COST_TYPE', default='score',\n",
    "                        type=str, help='cost function type to use for matching, other options are scoreiou, iou')\n",
    "    parser.add_argument('--PATHS_JUMP_GAP', default=4,\n",
    "                        type=int, help='GAP allowed for a tube to be kept alive after no matching detection found')\n",
    "    parser.add_argument('--PATHS_MIN_LEN', default=6,\n",
    "                        type=int, help='minimum length of generated path')\n",
    "    parser.add_argument('--PATHS_MINSCORE', default=0.1,\n",
    "                        type=float, help='minimum score a path should have over its length')\n",
    "    \n",
    "    ## paths hyper parameters\n",
    "    parser.add_argument('--COMPUTE_TUBES', default=False, type=str2bool, help='if set true then it overwrite existing tubes')\n",
    "    parser.add_argument('--TUBES_ALPHA', default=0,\n",
    "                        type=float, help='alpha cost for changeing the label')\n",
    "    parser.add_argument('--TRIM_METHOD', default='none',\n",
    "                        type=str, help='other one is indiv which works for UCF24')\n",
    "    parser.add_argument('--TUBES_TOPK', default=10,\n",
    "                        type=int, help='Number of labels to assign for a tube')\n",
    "    parser.add_argument('--TUBES_MINLEN', default=5,\n",
    "                        type=int, help='minimum length of a tube')\n",
    "    parser.add_argument('--TUBES_EVAL_THRESHS', default='0.2,0.5',\n",
    "                        type=str, help='evaluation threshold for checking tube overlap at evaluation time, one can provide as many as one wants')\n",
    "    # parser.add_argument('--TRAIL_ID', default=0,\n",
    "    #                     type=int, help='eval TUBES_Thtrshold at evaluation time')\n",
    "    \n",
    "    ###\n",
    "    parser.add_argument('--LOG_START', default=10, \n",
    "                        type=int, help='start loging after k steps for text/tensorboard') \n",
    "    parser.add_argument('--LOG_STEP', default=10, \n",
    "                        type=int, help='Log every k steps for text/tensorboard')\n",
    "    parser.add_argument('--TENSORBOARD', default=1,\n",
    "                        type=str2bool, help='Use tensorboard for loss/evalaution visualization')\n",
    "\n",
    "    # Program arguments\n",
    "    parser.add_argument('--MAN_SEED', default=123, \n",
    "                        type=int, help='manualseed for reproduction')\n",
    "    parser.add_argument('--MULTI_GPUS', default=True, type=str2bool, help='If  more than 0 then use all visible GPUs by default only one GPU used ') \n",
    "\n",
    "    # Use CUDA_VISIBLE_DEVICES=0,1,4,6 to select GPUs to use\n",
    "\n",
    "\n",
    "    ## Parse arguments\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    args = utils.set_args(args) # set directories and SUBSETS fo datasets\n",
    "    args.MULTI_GPUS = False if args.BATCH_SIZE == 1 else args.MULTI_GPUS\n",
    "    ## set random seeds and global settings\n",
    "    np.random.seed(args.MAN_SEED)\n",
    "    torch.manual_seed(args.MAN_SEED)\n",
    "    # torch.cuda.manual_seed_all(args.MAN_SEED)\n",
    "    torch.set_default_tensor_type('torch.FloatTensor')\n",
    "\n",
    "    args = utils.create_exp_name(args)\n",
    "\n",
    "    utils.setup_logger(args)\n",
    "    logger = utils.get_logger(__name__)\n",
    "    logger.info(sys.version)\n",
    "\n",
    "    assert args.MODE in ['train','val','gen_dets','eval_frames', 'eval_tubes'], 'MODE must be from ' + ','.join(['train','test','tubes'])\n",
    "\n",
    "    if args.MODE == 'train':\n",
    "        args.TEST_SEQ_LEN = args.SEQ_LEN\n",
    "    else:\n",
    "        args.SEQ_LEN = args.TEST_SEQ_LEN\n",
    "\n",
    "    if args.MODE in ['train','val']:\n",
    "        # args.CONF_THRESH = 0.05\n",
    "        args.SUBSETS = args.TRAIN_SUBSETS\n",
    "        train_transform = transforms.Compose([\n",
    "                            vtf.ResizeClip(args.MIN_SIZE, args.MAX_SIZE),\n",
    "                            vtf.ToTensorStack(),\n",
    "                            vtf.Normalize(mean=args.MEANS, std=args.STDS)])\n",
    "        \n",
    "        # train_skip_step = args.SEQ_LEN\n",
    "        # if args.SEQ_LEN>4 and args.SEQ_LEN<=10:\n",
    "        #     train_skip_step = args.SEQ_LEN-2\n",
    "\n",
    "        if args.SEQ_LEN>10:\n",
    "            train_skip_step = args.SEQ_LEN + (args.MAX_SEQ_STEP - 1) * 2 - 2\n",
    "        else:\n",
    "            train_skip_step = args.SEQ_LEN \n",
    "\n",
    "        args.DATASET = 'road'\n",
    "        train_s_dataset = VideoDataset(args, train=True, skip_step=train_skip_step, transform=train_transform)\n",
    "        logger.info('Done Loading Source ({}) Train Dataset'.format(args.DATASET))\n",
    "\n",
    "        args.DATASET = 'roadpp'\n",
    "        args.SUBSETS = ['train']\n",
    "        trains_t_dataset = VideoDataset(args, train=True, skip_step=train_skip_step, transform=train_transform)\n",
    "        logger.info('Done Loading Target ({}) Train Dataset'.format(args.DATASET))\n",
    "\n",
    "        train_st_dataset = ConcatDataset([train_s_dataset, trains_t_dataset])\n",
    "\n",
    "        logger.info('Train {} Dataset Size: {}'.format('road', len(train_s_dataset)))\n",
    "        logger.info('Train {} Dataset Size: {}'.format('roadpp', len(trains_t_dataset)))\n",
    "        logger.info('Train {} Dataset Size: {}'.format('road', len(train_st_dataset)))\n",
    "        ## For validation set\n",
    "        full_test = False\n",
    "        args.SUBSETS = args.VAL_SUBSETS\n",
    "        skip_step = args.SEQ_LEN*4\n",
    "    else:\n",
    "        args.SEQ_LEN = args.TEST_SEQ_LEN\n",
    "        args.MAX_SEQ_STEP = 1\n",
    "        args.SUBSETS = args.TEST_SUBSETS\n",
    "        full_test = True #args.MODE != 'train'\n",
    "        args.skip_beggning = 0\n",
    "        args.skip_ending = 0\n",
    "        if args.MODEL_TYPE == 'I3D' or 'SlowFast':\n",
    "            args.skip_beggning = 2\n",
    "            args.skip_ending = 2\n",
    "        elif args.MODEL_TYPE != 'C2D':\n",
    "            args.skip_beggning = 2\n",
    "\n",
    "        skip_step = args.SEQ_LEN - args.skip_beggning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
